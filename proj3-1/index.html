<html>
	<head>
	</head>
	<body>
		<h1> Project 3-1 </h1>
		<p>Overview:<br>
		   
		</p>
		<p></p>
		<p> Task1:<br>
		    To begin with, the ray generation required a couple transformations from the normalized image space, to camera space, and then to world space. First, <br>
		    we notice that from image space to camera space, the center of the viewing window goes from (0.5, 0.5) in the x, y coordinates to (0, 0) in the x, y <br>
		    coordinates, so we can see that a translation of (-0.5, -0.5) happens. Also, the viewing window is 2 * (tan(0.5 * hFov)) wider and 2 * (tan(0.5 * vFov) <br>
		    taller, so we also know to use those values for the scaling. We put these values into separate translation and scaling matrices and then multiply them <br>
		    together in order to get the transformation matrix for image space to camera space. Using this transformation matrix, we can multiply the normalized <br>
		    image coordinates (with a 1 for its z-coordinate) in order to get the image coordinate transformed to camera space. Finally we set the result's z coord <br>
		    to -1, since it will lie in the virtual camera sensor. Normalizing this vector, we can use it as the direction vector for our camera ray, with the pos <br>
		    of the camera used to define the origin of the ray in world space. Next, we just multiply the direction vector with the c2w matrix in order to transform <br>
		    it from camera space to world space, which ultimately gives us the ray in world space in conjunction with the camera's world position defined in pos. <br>
		    <br>
	            To estimate the illumination of a pixel, we use Monte Carlo integration, getting ns_aa number of samples from a random position in the bounds of the <br>
		    pixel's (x + [0,1],y+ [0,1]). We scale down this ocoordinate by dividing its x by sampleBuffer.w and y by sampleBuffer.h to put it into normalized image <br>
		    space, and then pass this coordinate into the ray generation function, which is then used in the global illumination estimate function. This result is added <br>
		    to the sum, which is divided at the end by ns_aa to calculate the approximation of the illumination of the pixel. <br>
		    <br>
		    For triangle intersection, I used the MÃ¶ller Trumbore algorithm to calclulate the intersection of the triangle. By following the equations laid out in the <br>
		    lecture, the calculations ultimately give you a t, which is the time of intersection with the plane that the triangle lies in, b1, b2, and b0 = 1 - b1 - b2. <br>
		    b0, b1, and b2 represent barycentric coordinates of the triangle, so we can check if the triangle is intersected by the ray if 1. the t value lies within the <br>
		    min and max t of the ray, and 2, all the barycentric coordinates are positive. If it does in fact intersect, we can populate the values of the provided Intersection <br>
		    with the appropriate values, the most notable of which is the normal coordinate which is calculated using barycentric interpolation with the b's and the normals <br>
		    at the points of the triangle.<br>
		    <br>
		    Sphere intersection was implemented by once again following the equations provided in lecture. At the end, the calculations result in two t values from solving <br>
		    the quadratic equation (if the quadratic equation results in a negative square root, we say there is no intersection). Starting with the smaller t value (which <br>
		    implies a closer intersection), we check if the v is within the range of the ray's min and max t, and if it is, we populate Intersection appropriatley, calculating <br>
		    the normal by getting the ray from the sphere's center to the point of intersection and then normalizing that. Also of note is that in both sphere and triangle <br>
		    intersection, we're updating the ray's max_t to be the latest point of intersection so that we are never rendering something that is further behind something we've <br>
		    already intersected.
		</p>
		<p>Spheres</p>
		<img src="spheres.png" alt="spheres">
		<p>Gems</p>
		<img src="task1gems.png" alt="gems">
		<p>Cow</p>
	        <img src="task1cow.png" alt="cow">
		<p>Banana</p>
		<img src="task1banana.png" alt="banana">
		<p></p>
		<p>Task 2: <br>
		   The bvh construction algorithm that I used is a recursive algorithm. It starts by establishing a bounding box for the node currently being created using nodes encompassed <br>
		   by the input start and end. It then creates a new bvh node using this bounding box. The next step is determined by whether this current node is a leaf node or not, which <br>
		   is determined by how many primitives are in the node, and is checked by seeing if the node's end - start is less than or equal to the max leaf size. If it is, then the <br>
		   node's start and end are set to the input start and end, its left and right children are set to null, and the node is returned, ending the recursion. Otherwise, the <br>
		   node is not a leaf node and needs to be further partitioned. To do this, it first decides what the longest axis is by checking the extent of the bounding box object. <br>
		   This longest axis will be chosen as the axis to be splitting upon. For choosing the actual split point on this axis, it takes the average position on this axis of all <br>
		   the primitives in this bounding box and uses this as the splitting point. (Originally I used the midpoint of the bounding box on the axis as the split point, but this lead <br>
		   to a stack overflow on generating the bvh for CBlucy.) After getting the split point, it uses std::partition to partition within the range of start and end, where what side <br>
		   of the partition a primitive is on is determined by comparing that primitive's centroid's position on the axis to the split point, placing the primitive in one partition if <br>
		   it's lower than the split point, and the other partition if it is higher. With the primitives partitioned, the next recursive is started by assigning the current node's <br>
		   left child to another call of this function, except only ranging from the input start point to the newly calculated mid point. Similarly, the right child is assigned to <br>
		   another call of the function with a range from the mid primitive to the end. To cover for the edge case where a partition would leave one partition empty, at a minimum, <br>
		   the start primitive is always placed into one partition, while the end primitive is always placed into the other one.
		</p>
		<p>Peter</p>
		<img src="task2peter.png" alt="peter">
		<p>MaxPlanck</p>
		<img src="task2maxplanck.png" alt="maxplanck">
		<p>Lucy</p>
	        <img src="task2lucy.png" alt="lucy">
		<p></p>
		<p> Rendered with 8 cores and resolution of 800 x 600: <br>
		    <br>
		    Banana: <br>
		    Without bvh: 13.4015s <br>
		    With bvh: 0.0987s <br>
					    <br>
		    Cow: <br>
		    Without bvh: 31.9632s <br>
		    With bvh: 0.1180 <br>
					    <br>
		    Bunny: <br>
		    Without bvh: 269.5522s <br>
		    With bvh: 0.1459s <br>
		    <br>
		    There's an obviously massive speed up here in all the cases. Importantly, the rendering time does not increase nearly as much in relation to the number of primitives <br>
		    compared to without bvh acceleration. As expected, it seems that the rendering goes from ~O(N) in relation to primitives to about O(log2(N)). This can be seen in the <br>
		    cow and bunny render, where the cow has about 5000 primitives, and gets a speed up of around 5000/log2(5000). The bunny render has around 33000 primitives and gets a <br>
		    similar result. 
		</p>
		
		
		    
	</body>
</html>
